{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymatgen as mg\n",
    "from matminer.featurizers.conversions import StrToComposition\n",
    "from matminer.featurizers.composition import ElementProperty\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load list of ABC compositions from the ICSD, Springer Materials, and ASM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_kfold = True\n",
    "n_trials = 10\n",
    "trees = 100\n",
    "stability_threshold = 0\n",
    "\n",
    "training_data_filename = 'training_data.csv'\n",
    "original_data = pd.read_csv(f'../released_Data Files/{training_data_filename}')\n",
    "\n",
    "output_filename = f'ML_predictions_{training_data_filename}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolate category I and II compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_II = original_data['Type'] == 'I_II'\n",
    "III_IV = original_data['Type'] == 'III_IV'\n",
    "\n",
    "I_II_on_hull = []\n",
    "\n",
    "for i, row in original_data[I_II].iterrows():\n",
    "    e1 = row['EP 1 Ehull (eV/atom)'] # DFT stability of first HH configuration\n",
    "    e2 = row['EP 2 Ehull (eV/atom)']\n",
    "    e3 = row['EP 3 Ehull (eV/atom)']\n",
    "    \n",
    "    stability = min([e1,e2,e3])\n",
    "    \n",
    "    if stability <= stability_threshold:\n",
    "        I_II_on_hull += [True]\n",
    "    else:\n",
    "        I_II_on_hull += [False]\n",
    "    \n",
    "I_II_on_hull = np.array(I_II_on_hull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolate category III and IV compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "III_IV_on_hull = []\n",
    "\n",
    "for i, row in original_data[III_IV].iterrows():\n",
    "    e1 = row['EP 1 Ehull (eV/atom)']\n",
    "    e2 = row['EP 2 Ehull (eV/atom)']\n",
    "    e3 = row['EP 3 Ehull (eV/atom)']\n",
    "    \n",
    "    stability = min([e1,e2,e3])\n",
    "    \n",
    "    if stability <= stability_threshold:\n",
    "        III_IV_on_hull += [True]\n",
    "    else:\n",
    "        III_IV_on_hull += [False]\n",
    "III_IV_on_hull = np.array(III_IV_on_hull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort data into the four categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = original_data[I_II][I_II_on_hull]\n",
    "II = original_data[I_II][~I_II_on_hull]\n",
    "III = original_data[III_IV][III_IV_on_hull]\n",
    "IV = original_data[III_IV][~III_IV_on_hull]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First define a quick helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.calculatoratoz.com/en/percent-ionic-character-calculator/Calc-900#:~:text=Percent%20ionic%20character%3F-,Percent%20ionic%20character%20is%20a%20measure%20of%20compound%20or%20molecule%27s,element%20B)%5E2)).\n",
    "\n",
    "def cubical_octahedral_differences_and_elementals(t1,o1,o2):\n",
    "    \n",
    "    t1_o1o2 = np.subtract(t1,np.mean((o1,o2),axis=0))\n",
    "    \n",
    "    return t1_o1o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "StrToComposition: 100%|██████████| 106/106 [00:00<00:00, 1837.48it/s]\n",
      "ElementProperty: 100%|██████████| 106/106 [00:00<00:00, 1074.66it/s]\n",
      "StrToComposition: 100%|██████████| 106/106 [00:00<00:00, 1969.45it/s]\n",
      "ElementProperty: 100%|██████████| 106/106 [00:00<00:00, 1204.82it/s]\n",
      "StrToComposition: 100%|██████████| 106/106 [00:00<00:00, 1875.44it/s]\n",
      "ElementProperty: 100%|██████████| 106/106 [00:00<00:00, 1020.33it/s]\n",
      "StrToComposition: 100%|██████████| 106/106 [00:00<00:00, 1921.65it/s]\n",
      "ElementProperty: 100%|██████████| 106/106 [00:00<00:00, 1162.54it/s]\n",
      "StrToComposition: 100%|██████████| 43/43 [00:00<00:00, 828.89it/s]\n",
      "ElementProperty: 100%|██████████| 43/43 [00:00<00:00, 782.97it/s]\n",
      "StrToComposition: 100%|██████████| 43/43 [00:00<00:00, 855.36it/s]\n",
      "ElementProperty: 100%|██████████| 43/43 [00:00<00:00, 793.59it/s]\n",
      "StrToComposition: 100%|██████████| 43/43 [00:00<00:00, 784.69it/s]\n",
      "ElementProperty: 100%|██████████| 43/43 [00:00<00:00, 806.32it/s]\n",
      "StrToComposition: 100%|██████████| 43/43 [00:00<00:00, 852.78it/s]\n",
      "ElementProperty: 100%|██████████| 43/43 [00:00<00:00, 705.67it/s]\n",
      "StrToComposition: 100%|██████████| 41/41 [00:00<00:00, 855.66it/s]\n",
      "ElementProperty: 100%|██████████| 41/41 [00:00<00:00, 784.27it/s]\n",
      "StrToComposition: 100%|██████████| 41/41 [00:00<00:00, 752.02it/s]\n",
      "ElementProperty: 100%|██████████| 41/41 [00:00<00:00, 709.78it/s]\n",
      "StrToComposition: 100%|██████████| 41/41 [00:00<00:00, 803.26it/s]\n",
      "ElementProperty: 100%|██████████| 41/41 [00:00<00:00, 787.17it/s]\n",
      "StrToComposition: 100%|██████████| 41/41 [00:00<00:00, 822.55it/s]\n",
      "ElementProperty: 100%|██████████| 41/41 [00:00<00:00, 680.35it/s]\n",
      "StrToComposition: 100%|██████████| 2200/2200 [00:02<00:00, 1004.89it/s]\n",
      "ElementProperty: 100%|██████████| 2200/2200 [00:23<00:00, 95.12it/s] \n",
      "StrToComposition: 100%|██████████| 2200/2200 [00:02<00:00, 970.63it/s] \n",
      "ElementProperty: 100%|██████████| 2200/2200 [00:21<00:00, 100.06it/s]\n",
      "StrToComposition: 100%|██████████| 2200/2200 [00:02<00:00, 1050.59it/s]\n",
      "ElementProperty: 100%|██████████| 2200/2200 [00:21<00:00, 100.42it/s]\n",
      "StrToComposition: 100%|██████████| 2200/2200 [00:03<00:00, 731.32it/s] \n",
      "ElementProperty: 100%|██████████| 2200/2200 [00:21<00:00, 100.64it/s]\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "\n",
    "for data in [I,II,III,IV]:\n",
    "\n",
    "    formulas = pd.DataFrame(data['Composition'])\n",
    "\n",
    "    # 1. Featurize each individual element in the Half Heusler composition (in increasing order of electropositivity - EP))\n",
    "    \n",
    "    ep_feat = ElementProperty.from_preset(preset_name=\"magpie\")\n",
    "    e_feat = ElementProperty.from_preset(preset_name=\"magpie\")\n",
    "    ep_1_comps = StrToComposition().featurize_dataframe(pd.DataFrame(data['EP 1 element']), 'EP 1 element')\n",
    "    ep_1_features = e_feat.featurize_dataframe(ep_1_comps, col_id=\"composition\")\n",
    "    ep_2_comps = StrToComposition().featurize_dataframe(pd.DataFrame(data['EP 2 element']), 'EP 2 element')\n",
    "    ep_2_features = e_feat.featurize_dataframe(ep_2_comps, col_id=\"composition\").to_numpy()[:,2:]\n",
    "    ep_3_comps = StrToComposition().featurize_dataframe(pd.DataFrame(data['EP 3 element']), 'EP 3 element')\n",
    "    ep_3_features = e_feat.featurize_dataframe(ep_3_comps, col_id=\"composition\").to_numpy()[:,2:]\n",
    "    all_ep_features = np.concatenate([ep_1_features.to_numpy()[:,2:].reshape((len(ep_1_features),len(ep_2_features[0]),1)),\\\n",
    "                                    ep_2_features.reshape((len(ep_1_features),len(ep_2_features[0]),1)),\\\n",
    "                                    ep_3_features.reshape((len(ep_1_features),len(ep_2_features[0]),1))],axis=2)\n",
    "    \n",
    "    # 2. identify cubical and octahedral elements, get their respective stabilities\n",
    "    \n",
    "    cubical_indices = []\n",
    "    octahedral_indices = []\n",
    "    stabilities = []\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        all_stabilities = row[['EP 1 Ehull (eV/atom)', 'EP 2 Ehull (eV/atom)','EP 3 Ehull (eV/atom)']].values\n",
    "        stability_index = np.argmin(all_stabilities)\n",
    "        cubical_indices += [stability_index]\n",
    "        \n",
    "        stabilities += [all_stabilities[stability_index]]\n",
    "        all_indices = np.array([0,1,2])\n",
    "        octahedral_indices += [all_indices[np.isin(all_indices,[stability_index],invert=True)]]\n",
    "\n",
    "    cubicals = []\n",
    "    octahedrals_1 = []\n",
    "    octahedrals_2 = []\n",
    "\n",
    "    for i in range(len(all_ep_features)):\n",
    "        cubicals += [all_ep_features[i][:,cubical_indices[i]]]\n",
    "        octahedrals_1 += [all_ep_features[i][:,octahedral_indices[i][0]]]\n",
    "        octahedrals_2 += [all_ep_features[i][:,octahedral_indices[i][1]]]\n",
    "    \n",
    "    cubicals = np.array(cubicals)\n",
    "    octahedrals_1 = np.array(octahedrals_1)\n",
    "    octahedrals_2 = np.array(octahedrals_2)\n",
    "    \n",
    "    octahedral_avg = np.mean([octahedrals_1,octahedrals_2],axis=0)\n",
    "    stabilities = np.array(stabilities)\n",
    "    \n",
    "    # 3. featurize the entire Half Heusler formula\n",
    "    \n",
    "    formulas_comps = StrToComposition().featurize_dataframe(formulas, 'Composition')\n",
    "    formula_features = ep_feat.featurize_dataframe(formulas_comps, col_id=\"composition\")\n",
    "    \n",
    "    # 4. calculate cubical-octahedral features\n",
    "    \n",
    "    cub_oct_diff = cubical_octahedral_differences_and_elementals(cubicals,octahedrals_1,octahedrals_2)\n",
    "    \n",
    "    # 5. combine to get all features\n",
    "    combined_features = np.concatenate([np.reshape(stabilities,(len(stabilities),1)),formula_features.to_numpy()[:,2:],cub_oct_diff,cubicals,octahedral_avg],axis=1)\n",
    "    all_features += [combined_features]\n",
    "\n",
    "cubical_octahedral_feature_names = []\n",
    "cubical_feature_names = []\n",
    "octahedral_feature_names = []\n",
    "\n",
    "# rename some feature names\n",
    "for x in ep_1_features.columns[2:]:\n",
    "    cubical_octahedral_feature_names += ['cub_oct_'+x]\n",
    "    cubical_feature_names += ['cub_'+x]\n",
    "    octahedral_feature_names += ['oct_'+x]\n",
    "\n",
    "# get final list of feature names\n",
    "training_feature_names = np.concatenate([['Stability'],formula_features.columns[2:],cubical_octahedral_feature_names,cubical_feature_names,octahedral_feature_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to get the electron count of a composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def electronCount(chemFormula):\n",
    "    # takes in a string composition\n",
    "    \n",
    "    chemFormula = mg.core.composition.Composition(chemFormula)\n",
    "    \n",
    "    symbolsOnly = list(chemFormula.keys())\n",
    "    stoichOnly = list(chemFormula.values())\n",
    "    \n",
    "    electronCount = 0\n",
    "    \n",
    "    for i in range(len(symbolsOnly)):\n",
    "        a = mg.core.Element(symbolsOnly[i])\n",
    "        valence = a.full_electronic_structure[-2:]\n",
    "        \n",
    "        if a.is_rare_earth_metal: \n",
    "            electrons = 3\n",
    "        else:\n",
    "            electrons = int(valence[0][2]) + int(valence[1][2])\n",
    "            \n",
    "        electronCount += electrons * stoichOnly[i]\n",
    "\n",
    "    electronCount /= max(stoichOnly)\n",
    "\n",
    "    return electronCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Define function to run ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_ML(n_undersample,trial_number,n_trees,aggregated_feature_importances,training_feature_names):\n",
    "    \n",
    "    data_list = [I,II,III,IV]\n",
    "    all_compositions = np.concatenate([data['Composition'].to_numpy() for data in data_list])\n",
    "    training_labels = np.concatenate([[0]*len(I),[0]*len(II),[1]*len(III),[1]*len(IV)]) # 0 = synthesizable, 1 = unsynthesizable  \n",
    "    \n",
    "    training_features = np.concatenate(all_features)\n",
    "    \n",
    "    # find NaNs\n",
    "\n",
    "    nan_counter_examples = 0\n",
    "\n",
    "    nan_indices = [0]*training_features.shape[1]\n",
    "    for examples in training_features:\n",
    "        nan_counter = 0\n",
    "        for i in range(len(examples)):\n",
    "            if np.isnan(examples[i]): \n",
    "                nan_indices[i] += 1\n",
    "                nan_counter +=1\n",
    "        if nan_counter > 0: nan_counter_examples += 1\n",
    "\n",
    "\n",
    "    # remove NaNs and impute the rest\n",
    "\n",
    "    indices_to_remove = []\n",
    "    for i in range(len(nan_indices)):\n",
    "        if nan_indices[i] >= 10: #if more than 10 NaNs in the feature, remove the feature\n",
    "            indices_to_remove += [i]\n",
    "            print(f'Removing NaN feature -> {training_feature_names[i]}.')\n",
    "\n",
    "    training_features = np.delete(training_features, indices_to_remove, axis=1)\n",
    "    training_feature_names = np.delete(training_feature_names, indices_to_remove)\n",
    "    \n",
    "    \n",
    "    # impute features with fewer than 10 NaNs\n",
    "\n",
    "    imputer_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputer_mean.fit(training_features)\n",
    "    training_features = imputer_mean.transform(training_features)    \n",
    "    \n",
    "    # undersample category IV data points and keep the non-training data to predict on later\n",
    "\n",
    "    remove_IV = np.random.choice(range(len(I)+len(II)+len(III),len(I)+len(II)+len(III)+len(IV)),size=(len(IV)-n_undersample),replace=False)\n",
    "    remove_IV = np.concatenate((remove_IV,range(len(I)+len(II)+len(III)+len(IV),len(training_features))))\n",
    "\n",
    "    remove_IV = remove_IV.astype(dtype=int)\n",
    "    \n",
    "    keep_IV = list(set(range(len(training_features))).difference(set(remove_IV)))\n",
    "    \n",
    "    remaining_features = training_features[remove_IV]\n",
    "    predicted_comps = all_compositions[remove_IV]\n",
    "\n",
    "    training_features = training_features[keep_IV]\n",
    "    training_labels = training_labels[keep_IV]\n",
    "    training_compositions = all_compositions[keep_IV]\n",
    "\n",
    "    \n",
    "    # run ML\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators = n_trees,n_jobs=-1)\n",
    " \n",
    "    splits = int(len(training_labels)/10) # leave ten out\n",
    "    #splits = int(len(training_labels)) # leave one out\n",
    "    kf = KFold(n_splits = splits,shuffle=True) \n",
    "\n",
    "    training_predicted = []\n",
    "    training_predicted_comps = []\n",
    "\n",
    "\n",
    "    # important to convert to numpy array\n",
    "    training_features = np.array(training_features)\n",
    "    training_labels = np.array(training_labels)\n",
    "    training_compositions = np.array(training_compositions)\n",
    "    \n",
    "    for train_index, test_index in kf.split(training_features,training_labels):\n",
    "        X_train, X_test = training_features[train_index], training_features[test_index]\n",
    "        y_train, y_test = training_labels[train_index], training_labels[test_index]\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_predicted = model.predict_proba(X_test)\n",
    "\n",
    "        training_predicted += [y_predicted]\n",
    "        training_predicted_comps += [training_compositions[test_index]]\n",
    "\n",
    "    train_predicted , train_predicted_comps = np.concatenate(training_predicted,axis=0),np.concatenate(training_predicted_comps)\n",
    "    \n",
    "    # still need to predict on remaining data points that were thrown out from undersampling\n",
    "    model.fit(training_features,training_labels)\n",
    "        \n",
    "    try:\n",
    "        predicted = np.array(model.predict_proba(remaining_features))\n",
    "\n",
    "    except: # in the case that there's no undersampling\n",
    "        predicted = None\n",
    "        pass\n",
    "    \n",
    "    # get feature importances\n",
    "    sorted_feature_importance_indices = np.argsort(model.feature_importances_)\n",
    "    sorted_feature_importances = np.array(model.feature_importances_)[sorted_feature_importance_indices]\n",
    "    sorted_feature_names = np.array(training_feature_names)[sorted_feature_importance_indices]\n",
    "\n",
    "    ranked_features = np.vstack([sorted_feature_importances,sorted_feature_names]).T\n",
    "    \n",
    "    \n",
    "    for i in range(len(ranked_features)):\n",
    "        aggregated_feature_importances[ranked_features[i,1]] += [ranked_features[i,0]]\n",
    "    \n",
    "    return predicted, predicted_comps, train_predicted, train_predicted_comps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ML trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trial 1.\n",
      "Running trial 2.\n",
      "Running trial 3.\n",
      "Running trial 4.\n",
      "Running trial 5.\n",
      "Running trial 6.\n",
      "Running trial 7.\n",
      "Running trial 8.\n",
      "Running trial 9.\n",
      "Running trial 10.\n",
      "Finished in 79.6895580291748 seconds.\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "\n",
    "all_kfold_predictions = defaultdict(list)\n",
    "n_undersample = 600\n",
    "\n",
    "aggregated_feature_importances = defaultdict(list)\n",
    "\n",
    "for i in range(n_trials):\n",
    "    print(f'Running trial {i+1}.')\n",
    "    # use these lines if predicting probabilities\n",
    "    predicted, predicted_comps, training_predicted_probs, training_predicted_comps = run_ML(n_undersample,n_trees = trees,trial_number = i,training_feature_names = training_feature_names,aggregated_feature_importances = aggregated_feature_importances)\n",
    "    \n",
    "    try:        \n",
    "        predicted_probabilities_synthesis = predicted_probs[:,0] \n",
    "\n",
    "        for i in range(len(predicted_probs)):\n",
    "            all_kfold_predictions[predicted_comps[i]] += [predicted_probs[i]]\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        for i in range(len(training_predicted_probs)):\n",
    "            all_kfold_predictions[training_predicted_comps[i]] += [training_predicted_probs[i]]\n",
    "    except:\n",
    "        pass\n",
    "b = time.time()\n",
    "print(f'Finished in {b-a} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate feature importances and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 529 features total.\n",
      "                                     Name  Importances\n",
      "528                             Stability     0.055603\n",
      "526    MagpieData maximum MendeleevNumber     0.022912\n",
      "525        MagpieData mode CovalentRadius     0.022668\n",
      "527     MagpieData minimum CovalentRadius     0.022665\n",
      "524        MagpieData avg_dev GSvolume_pa     0.011175\n",
      "522            MagpieData mean NpUnfilled     0.009652\n",
      "523     MagpieData avg_dev CovalentRadius     0.009552\n",
      "514       MagpieData range CovalentRadius     0.008932\n",
      "521  cub_oct_MagpieData minimum NpValence     0.008816\n",
      "498          MagpieData avg_dev NpValence     0.008628\n"
     ]
    }
   ],
   "source": [
    "means = []\n",
    "\n",
    "for key in aggregated_feature_importances.keys():\n",
    "    importances = np.array(aggregated_feature_importances[key],dtype=float)\n",
    "    means += [np.mean(importances)]\n",
    "    \n",
    "feature_info = pd.DataFrame(data={'Name':list(aggregated_feature_importances.keys()),\n",
    "                                 'Importances':means}).sort_values(by='Importances',ascending=False,inplace=False)\n",
    "\n",
    "print(f'There are {len(feature_info)} features total.')\n",
    "print(feature_info.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate probabilities from n trials and write results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_comps = []\n",
    "results_synth_probs = []\n",
    "results_e_hull = []\n",
    "results_experimental = []\n",
    "results_e_count = []\n",
    "results_band_gap = []\n",
    "\n",
    "\n",
    "\n",
    "for comp in all_kfold_predictions:\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # record E hull\n",
    "\n",
    "        e1 = original_data[original_data['Composition'] == comp]['EP 1 Ehull (eV/atom)'].values[0]\n",
    "        e2 = original_data[original_data['Composition'] == comp]['EP 2 Ehull (eV/atom)'].values[0]\n",
    "        e3 = original_data[original_data['Composition'] == comp]['EP 3 Ehull (eV/atom)'].values[0]\n",
    "    \n",
    "    except:\n",
    "        # record E hull\n",
    "        \n",
    "        e1 = predict_data[predict_data['Composition'] == comp]['EP 1 Ehull (eV/atom)'].values[0]\n",
    "        e2 = predict_data[predict_data['Composition'] == comp]['EP 2 Ehull (eV/atom)'].values[0]\n",
    "        e3 = predict_data[predict_data['Composition'] == comp]['EP 3 Ehull (eV/atom)'].values[0]\n",
    "\n",
    "    variant_energies = np.array([e1,e2,e3])\n",
    "    ehull_index = np.argmin(variant_energies)\n",
    "    ehull = variant_energies[ehull_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        exp = predict_data[predict_data['Composition'] == comp]['Experimental Structure'].values[0]\n",
    "    except:\n",
    "        exp = original_data[original_data['Composition'] == comp]['Experimental Structure'].values[0]\n",
    "        \n",
    "    results_e_hull += [ehull]    \n",
    "    # record composition\n",
    "    results_comps += [comp]\n",
    "    # record synth probability\n",
    "    comp_probs = np.array(all_kfold_predictions[comp])[:,0]\n",
    "    avg_prob = np.mean(comp_probs)\n",
    "    results_synth_probs += [avg_prob]\n",
    "    # record experimental result\n",
    "    results_experimental += [exp]\n",
    "    # record electron count\n",
    "    results_e_count += [electronCount(comp)]\n",
    "    \n",
    "    \n",
    "results = pd.DataFrame({'Composition': results_comps, 'Synthesizability': results_synth_probs, 'Half Heusler (F-43m)': results_e_hull, 'Experimental Structure': results_experimental, 'Electron Count': results_e_count} )\n",
    "results.sort_values(by=['Synthesizability'],ascending=False,inplace=True)\n",
    "results.to_csv(output_filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a2ce6feceabe16081239645c3ccff6386a8ef75e380f6aa10f5bd5dbcbdf266"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
