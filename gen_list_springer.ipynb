{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to read downloaded .cif files from Springer Materials, filter out compositions of non-interest, identify problematic/unusable entries, and write the composition and crystal structure(s) to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymatgen.core as mg\n",
    "import matplotlib as plt\n",
    "import glob\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pymatgen.io import cif as mg_cif\n",
    "from itertools import combinations\n",
    "from CifFile import ReadCif\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hh_comp_rearrange(comp): # rearrange compositions to match with standard HH naming convention \n",
    "    try:\n",
    "        elements = comp.keys()\n",
    "        groups = [x.group for x in elements]\n",
    "        stoichiometry = comp.values()\n",
    "        is_rare_earth = [x.is_rare_earth_metal for x in elements]\n",
    "\n",
    "        if True in is_rare_earth:\n",
    "            for i in range(len(is_rare_earth)):\n",
    "                if is_rare_earth[i] == True: groups[i] = -1 # move rare earths to the front of composition\n",
    "        ordered_pbin_comp = np.array(sorted(zip(groups,elements,stoichiometry)))\n",
    "        \n",
    "        ordered_elements = ordered_pbin_comp[:,1]\n",
    "        ordered_stoichiometry = ordered_pbin_comp[:,2]\n",
    "        \n",
    "        comp_str = [str(ordered_elements[i])+str(ordered_stoichiometry[i]) if ordered_stoichiometry[i] != 1 else str(ordered_elements[i]) for i in range(len(ordered_elements))]\n",
    "        \n",
    "        return ''.join(comp_str)\n",
    "    except:\n",
    "        return comp\n",
    "\n",
    "def check_cif_equivalent(fname1,fname2): # check for identical / equivalent cif files\n",
    "    cif1 = mg.Structure.from_file(fname1).as_dict()\n",
    "    cif2 = mg.Structure.from_file(fname2).as_dict()\n",
    "    \n",
    "    lat_1 = cif1['lattice']['matrix']\n",
    "    lat_2 = cif2['lattice']['matrix']\n",
    "    \n",
    "    vol_1 = cif1['lattice']['volume']\n",
    "    vol_2 = cif2['lattice']['volume']\n",
    "    \n",
    "\n",
    "    tolerance = 0.05 # relative tolerance\n",
    "    \n",
    "    # check if lattice parameters are similar\n",
    "    if set(np.isclose(lat_1,lat_2,rtol=tolerance).flatten()) != {True}: return False\n",
    "    \n",
    "    # check if volumes are similar\n",
    "    elif not np.isclose(vol_1,vol_2,rtol=tolerance): return False\n",
    "    \n",
    "    # check if number of species are the same\n",
    "    elif len(cif1['sites']) != len(cif2['sites']): return False\n",
    "    \n",
    "    # check if sublattices are similar\n",
    "    for i in range(len(cif1['sites'])):\n",
    "        for key in cif1['sites'][i].keys():\n",
    "            if key == 'xyz': continue # skip absolute coordinates\n",
    "            try: # for integer values\n",
    "                if set(np.isclose(cif1['sites'][i][key],cif2['sites'][i][key],atol=tolerance)) != {True}: \n",
    "                    return False\n",
    "            except: # for non integer values (like strings)\n",
    "                if cif1['sites'][i][key] != cif2['sites'][i][key]: return False\n",
    "    \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all downloaded cifs and create dictionary to write final list to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_cifs = glob.glob('previous_downloads/*')\n",
    "\n",
    "results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse through each cif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cif in tqdm(downloaded_cifs):\n",
    "    try:\n",
    "        mg_comp = mg.Structure.from_file(cif).composition\n",
    "        formula = mg_comp.reduced_formula\n",
    "        mg_comp = mg.Composition(formula)\n",
    "        # ignore compositions that do not have ABC stoichiometry or have unknown species in the cif file\n",
    "        dummyspecies = [type(x) for x in mg_comp.elements]\n",
    "        if set(mg_comp.values()) != {1.0} or mg.periodic_table.DummySpecie in dummyspecies or len(mg_comp.values()) != 3: continue\n",
    "\n",
    "        cif_obj = mg_cif.CifParser(cif)\n",
    "        cif_dict = cif_obj.as_dict()\n",
    "        cif_key = list(cif_dict.keys())[1]\n",
    "        hm_spacegroup = cif_dict[cif_key]['_symmetry_space_group_name_H-M']\n",
    "\n",
    "        # get errors from parsing cif file\n",
    "        error_status = cif_obj.has_errors\n",
    "\n",
    "        # try to get the prototype name\n",
    "        try:\n",
    "            cif_pycifrw = ReadCif(cif)\n",
    "            cif_prototype = cif_pycifrw['sm_global']['_sm_phase_prototype']\n",
    "        except:\n",
    "            cif_prototype = None\n",
    "    \n",
    "    except: # in the case of corrupt cif files\n",
    "        formula = cif\n",
    "        hm_spacegroup = None\n",
    "        error_status = 'FATAL'\n",
    "        cif_prototype = None\n",
    "        \n",
    "    # unify prototype names for easier processing\n",
    "\n",
    "    \n",
    "    try: cif_prototype = cif_prototype.replace(' ','')\n",
    "    except: pass\n",
    "    \n",
    "    if cif_prototype == None: pass\n",
    "    elif 'MgZn2' in cif_prototype and hm_spacegroup == \"P63/mmc\": cif_prototype = 'MgZn2'\n",
    "\n",
    "    results['Composition'] += [hh_comp_rearrange(formula)]\n",
    "    results['Space Group'] += [hm_spacegroup]\n",
    "    results['Errors'] += [error_status]\n",
    "    results['Filename'] += [cif.split('/')[1]]\n",
    "    results['Prototype'] += [cif_prototype]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write results to dataframe and check the dataframe for duplicate compositions and errors. Also, fix compositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "processed_results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through results for duplicate compositions and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covered_unprocessed_formulas = []\n",
    "\n",
    "for i, row in tqdm(df_results.iterrows(),total=len(df_results)):\n",
    "    \n",
    "    comp = row['Composition']\n",
    "    \n",
    "    # skip if composition has already been analyzed\n",
    "    if comp in covered_unprocessed_formulas: continue \n",
    "    else: covered_unprocessed_formulas += [comp]\n",
    "        \n",
    "    # skip corrupt cif files\n",
    "    if row['Errors'] == 'FATAL': \n",
    "        error_code = 7        \n",
    "        processed_results['Composition'] += [comp]\n",
    "        processed_results['Space Groups'] += ['ERROR']\n",
    "        processed_results['Replicates'] += ['ERROR']        \n",
    "        processed_results['Error Code'] += [error_code]\n",
    "        processed_results['Filename'] += [[row['Filename']]]\n",
    "        processed_results['Prototype'] += ['Error']\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # get all matches to the particular composition\n",
    "    matches = df_results.loc[df_results['Composition'] == comp]\n",
    "\n",
    "    # get set of space groups\n",
    "    match_space_groups = sorted(set(matches['Space Group']))\n",
    "    comp_new_filenames = []\n",
    "    comp_prototypes = []\n",
    "    \n",
    "    for sg in match_space_groups:\n",
    "        # find all instances of a given space group\n",
    "        sg_paths = np.array(matches[matches['Space Group'] == sg]['Filename'])\n",
    "        sg_prototypes = np.array(matches[matches['Space Group'] == sg]['Prototype'])\n",
    "        if len(sg_prototypes) == 0: sg_prototypes = [None]\n",
    "        comparison_combos = combinations(range(len(sg_paths)),2)\n",
    "        \n",
    "        # identify equivalent cifs\n",
    "        unique_prototypes = []\n",
    "        repeated_prototypes = []\n",
    "        \n",
    "        for indices in comparison_combos:\n",
    "            i0, i1 = indices[0], indices[1]\n",
    "            fname1 = f'previous_downloads/{sg_paths[i0]}'\n",
    "            fname2 = f'previous_downloads/{sg_paths[i1]}'\n",
    "            if not check_cif_equivalent(fname1,fname2): \n",
    "                unique_prototypes += [i for i in [i0,i1] if i not in repeated_prototypes]\n",
    "            else: \n",
    "                repeated_prototypes += [i1]\n",
    "        \n",
    "        unique_prototypes = list(set(unique_prototypes))\n",
    "        \n",
    "        # for compositions with only one associated file\n",
    "        if len(unique_prototypes) == 0: \n",
    "            new_filename = [sg_paths[0]]\n",
    "            prototype = [sg_prototypes[0]]\n",
    "        \n",
    "        # in case that not all cifs are the same prototype\n",
    "        else: \n",
    "            new_filename = list(sg_paths[unique_prototypes])\n",
    "            prototype = list(sg_prototypes[unique_prototypes])\n",
    "            # this next line implies that prototypes labeled the same name with the same space group are identical!\n",
    "            if len(list(set(prototype))) == 1: prototype = [prototype[0]]\n",
    "            \n",
    "        comp_new_filenames += [new_filename]\n",
    "        comp_prototypes += [prototype]\n",
    "        \n",
    "        \n",
    "    # determine the error code\n",
    "    ## 0 = no errors, single space group, single prototype\n",
    "    ## 1 = no errors, single space group, multiple prototypes\n",
    "    ## 2 = no errors, multiple space groups, (multiple prototypes)\n",
    "    ## 3 = errors with at least one .cif, single space group, single prototype\n",
    "    ## 4 = errors with at least one .cif, single space group, multiple prototypes\n",
    "    ## 5 =  errors with at least one .cif, multiple space groups, (multiple prototypes)\n",
    "    ## 6 = other error (there shouldn't be any 6's)\n",
    "    ## 7 = .cif file(s) couldn't be opened (FATAL)\n",
    "\n",
    "    if len(match_space_groups) == 1: \n",
    "        space_group_conflicts = False\n",
    "        \n",
    "        if len(comp_prototypes[0]) == 1: single_prototype = True\n",
    "        else: single_prototype = False\n",
    "    \n",
    "    else: space_group_conflicts = True\n",
    "    \n",
    "    if list(matches['Errors']).count(True) > 0: any_errors = True\n",
    "    else: any_errors = False\n",
    "\n",
    "        \n",
    "    if not space_group_conflicts and not any_errors and single_prototype: error_code = 0\n",
    "    elif not space_group_conflicts and not any_errors and not single_prototype: error_code = 1\n",
    "    elif space_group_conflicts and not any_errors: error_code = 2\n",
    "    elif not space_group_conflicts and any_errors and single_prototype: error_code = 3\n",
    "    elif not space_group_conflicts and any_errors and not single_prototype: error_code = 4\n",
    "    elif space_group_conflicts and any_errors: error_code = 5\n",
    "    else: error_code = 6\n",
    "\n",
    "    processed_results['Composition'] += [hh_comp_rearrange(mg.Composition(comp))]\n",
    "    processed_results['Space Groups'] += [list(match_space_groups)]\n",
    "    processed_results['Replicates'] += [len(matches)]    \n",
    "    processed_results['Error Code'] += [error_code]\n",
    "    processed_results['Filename'] += [str(comp_new_filenames)]\n",
    "    processed_results['Prototype'] += [str(comp_prototypes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write processed results to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_results = pd.DataFrame(processed_results)\n",
    "df_processed_results.to_csv('../Data Files/springer_results.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a2ce6feceabe16081239645c3ccff6386a8ef75e380f6aa10f5bd5dbcbdf266"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
